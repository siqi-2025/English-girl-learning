"""
æ–‡æ¡£ç”Ÿæˆæ¨¡å—

å°†åˆ†æç»“æœè½¬æ¢ä¸ºMarkdownæ ¼å¼çš„å­¦ä¹ æ–‡æ¡£
"""

import streamlit as st
import markdown
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Optional
import json


class MarkdownGenerator:
    """Markdownæ–‡æ¡£ç”Ÿæˆå™¨"""
    
    def __init__(self, output_dir: str = "output"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        # åˆ›å»ºå­ç›®å½•
        (self.output_dir / "lessons").mkdir(exist_ok=True)
        (self.output_dir / "vocabulary").mkdir(exist_ok=True)
        (self.output_dir / "exercises").mkdir(exist_ok=True)
    
    def generate_lesson_document(self, analysis_result: Dict, filename: Optional[str] = None) -> str:
        """
        ç”Ÿæˆè¯¾æ–‡æ–‡æ¡£
        
        Args:
            analysis_result: AIåˆ†æç»“æœ
            filename: è¾“å‡ºæ–‡ä»¶å
            
        Returns:
            ç”Ÿæˆçš„æ–‡æ¡£è·¯å¾„
        """
        analysis = analysis_result.get('analysis', {})
        unit = analysis.get('unit', 'Unknown')
        title = analysis.get('title', 'æœªå‘½åè¯¾æ–‡')
        
        if filename is None:
            filename = f"Unit{unit}_{title}.md".replace(' ', '_')
        
        # ç”ŸæˆMarkdownå†…å®¹
        content = self._create_lesson_markdown(analysis_result)
        
        # ä¿å­˜æ–‡ä»¶
        file_path = self.output_dir / "lessons" / filename
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(content)
        
        return str(file_path)
    
    def _create_lesson_markdown(self, analysis_result: Dict) -> str:
        """åˆ›å»ºè¯¾æ–‡Markdownå†…å®¹"""
        analysis = analysis_result.get('analysis', {})
        
        # æ„å»ºMarkdownæ–‡æ¡£
        md_content = []
        
        # æ ‡é¢˜
        unit = analysis.get('unit', 'Unknown')
        title = analysis.get('title', 'è‹±è¯­è¯¾æ–‡')
        md_content.append(f"# Unit {unit}: {title}")
        md_content.append("")
        
        # åŸºæœ¬ä¿¡æ¯
        md_content.append("## ğŸ“‹ è¯¾æ–‡ä¿¡æ¯")
        md_content.append("")
        md_content.append(f"- **å•å…ƒ**: Unit {unit}")
        md_content.append(f"- **æ ‡é¢˜**: {title}")
        md_content.append(f"- **ç±»å‹**: {analysis.get('content_type', 'Unknown')}")
        md_content.append(f"- **ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        md_content.append("")
        
        # è¯¾æ–‡å†…å®¹
        md_content.append("## ğŸ“– è¯¾æ–‡å†…å®¹")
        md_content.append("")
        
        # åŸå§‹OCRæ–‡æœ¬
        if analysis_result.get('raw_ocr'):
            md_content.append("### OCRè¯†åˆ«åŸæ–‡")
            md_content.append("```")
            md_content.append(analysis_result['raw_ocr'])
            md_content.append("```")
            md_content.append("")
        
        # AIæ ¡æ­£åæ–‡æœ¬
        if analysis_result.get('corrected_text'):
            md_content.append("### AIæ ¡æ­£åè¯¾æ–‡")
            md_content.append("")
            for line in analysis_result['corrected_text'].split('\n'):
                if line.strip():
                    md_content.append(f"> {line}")
            md_content.append("")
        
        # å†…å®¹æ¦‚è¿°
        if analysis.get('main_content'):
            md_content.append("### å†…å®¹æ¦‚è¿°")
            md_content.append("")
            md_content.append(analysis['main_content'])
            md_content.append("")
        
        # è¯æ±‡è¡¨
        vocabulary = analysis.get('vocabulary', [])
        if vocabulary:
            md_content.append("## ğŸ“š è¯æ±‡è¡¨")
            md_content.append("")
            md_content.append("| è‹±æ–‡å•è¯ | ä¸­æ–‡å«ä¹‰ | éš¾åº¦ç­‰çº§ | ä¾‹å¥ |")
            md_content.append("|----------|----------|----------|------|")
            
            for vocab in vocabulary:
                word = vocab.get('word', '')
                meaning = vocab.get('meaning', '')
                level = vocab.get('level', 'middle')
                example = vocab.get('example', '')
                level_emoji = "ğŸŸ¢" if level == "primary" else "ğŸŸ¡"
                md_content.append(f"| {word} | {meaning} | {level_emoji} {level} | {example} |")
            
            md_content.append("")
        
        # è¯­æ³•ç‚¹
        grammar_points = analysis.get('grammar_points', [])
        if grammar_points:
            md_content.append("## ğŸ“ è¯­æ³•è¦ç‚¹")
            md_content.append("")
            for i, point in enumerate(grammar_points, 1):
                md_content.append(f"{i}. {point}")
            md_content.append("")
        
        # AIæ ¡æ­£ä¿¡æ¯
        corrections = analysis_result.get('corrections', [])
        if corrections:
            md_content.append("## ğŸ”§ AIæ ¡æ­£è®°å½•")
            md_content.append("")
            md_content.append("| åŸæ–‡ | æ ¡æ­£å | æ ¡æ­£åŸå›  |")
            md_content.append("|------|--------|----------|")
            
            for correction in corrections:
                original = correction.get('original', '')
                corrected = correction.get('corrected', '')
                reason = correction.get('reason', '')
                md_content.append(f"| {original} | {corrected} | {reason} |")
            
            md_content.append("")
        
        # æ–‡æ¡£ä¿¡æ¯
        confidence = analysis_result.get('confidence', 0)
        md_content.append("## â„¹ï¸ æ–‡æ¡£ä¿¡æ¯")
        md_content.append("")
        md_content.append(f"- **è¯†åˆ«ç½®ä¿¡åº¦**: {confidence:.2%}")
        md_content.append(f"- **å¤„ç†æ–¹å¼**: AIå¢å¼ºOCR (PaddleOCR 3.1 + æ™ºæ™®AI)")
        md_content.append(f"- **ç”Ÿæˆå·¥å…·**: English Learning Assistant v1.1")
        
        return '\n'.join(md_content)
    
    def generate_vocabulary_document(self, vocabulary_data: Dict[str, List[Dict]], 
                                   filename: str = "vocabulary_summary.md") -> str:
        """
        ç”Ÿæˆè¯æ±‡æ±‡æ€»æ–‡æ¡£
        
        Args:
            vocabulary_data: è¯æ±‡åˆ†çº§æ•°æ®
            filename: è¾“å‡ºæ–‡ä»¶å
            
        Returns:
            ç”Ÿæˆçš„æ–‡æ¡£è·¯å¾„
        """
        md_content = []
        
        # æ ‡é¢˜
        md_content.append("# ğŸ“š è‹±è¯­è¯æ±‡æ±‡æ€»")
        md_content.append("")
        md_content.append(f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        md_content.append("")
        
        # ç»Ÿè®¡ä¿¡æ¯
        primary_count = len(vocabulary_data.get('primary', []))
        middle_count = len(vocabulary_data.get('middle', []))
        total_count = primary_count + middle_count
        
        md_content.append("## ğŸ“Š è¯æ±‡ç»Ÿè®¡")
        md_content.append("")
        md_content.append(f"- **æ€»è¯æ±‡é‡**: {total_count}")
        md_content.append(f"- **å°å­¦è¯æ±‡**: {primary_count} ä¸ª")
        md_content.append(f"- **ä¸­å­¦è¯æ±‡**: {middle_count} ä¸ª")
        md_content.append("")
        
        # å°å­¦è¯æ±‡
        primary_words = vocabulary_data.get('primary', [])
        if primary_words:
            md_content.append("## ğŸŸ¢ å°å­¦è¯æ±‡")
            md_content.append("")
            md_content.append("| å•è¯ | ä¸­æ–‡å«ä¹‰ | ä¾‹å¥ |")
            md_content.append("|------|----------|------|")
            
            for word_info in primary_words:
                word = word_info.get('word', '')
                meaning = word_info.get('meaning', '')
                example = word_info.get('example', '')
                md_content.append(f"| {word} | {meaning} | {example} |")
            
            md_content.append("")
        
        # ä¸­å­¦è¯æ±‡
        middle_words = vocabulary_data.get('middle', [])
        if middle_words:
            md_content.append("## ğŸŸ¡ ä¸­å­¦è¯æ±‡")
            md_content.append("")
            md_content.append("| å•è¯ | ä¸­æ–‡å«ä¹‰ | ä¾‹å¥ |")
            md_content.append("|------|----------|------|")
            
            for word_info in middle_words:
                word = word_info.get('word', '')
                meaning = word_info.get('meaning', '')
                example = word_info.get('example', '')
                md_content.append(f"| {word} | {meaning} | {example} |")
            
            md_content.append("")
        
        # å­¦ä¹ å»ºè®®
        md_content.append("## ğŸ’¡ å­¦ä¹ å»ºè®®")
        md_content.append("")
        md_content.append("### ğŸŸ¢ å°å­¦è¯æ±‡å­¦ä¹ æ–¹æ³•")
        md_content.append("- é‡ç‚¹æŒæ¡åŸºç¡€å«ä¹‰å’Œæ‹¼å†™")
        md_content.append("- é€šè¿‡ä¾‹å¥ç†è§£ä½¿ç”¨åœºæ™¯")
        md_content.append("- å¤šè¿›è¡Œå¬è¯´ç»ƒä¹ ")
        md_content.append("")
        
        md_content.append("### ğŸŸ¡ ä¸­å­¦è¯æ±‡å­¦ä¹ æ–¹æ³•")
        md_content.append("- ç†è§£è¯æ±‡çš„å¤šé‡å«ä¹‰")
        md_content.append("- å­¦ä¹ è¯æ±‡æ­é…å’Œå›ºå®šç”¨æ³•")
        md_content.append("- æ³¨æ„è¯æ€§å˜åŒ–å’Œè¯­æ³•åº”ç”¨")
        
        # ä¿å­˜æ–‡ä»¶
        file_path = self.output_dir / "vocabulary" / filename
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write('\n'.join(md_content))
        
        return str(file_path)
    
    def generate_exercise_document(self, exercises: Dict, unit: int = 1, 
                                 filename: Optional[str] = None) -> str:
        """
        ç”Ÿæˆç»ƒä¹ é¢˜æ–‡æ¡£
        
        Args:
            exercises: ä¹ é¢˜æ•°æ®
            unit: å•å…ƒç¼–å·
            filename: è¾“å‡ºæ–‡ä»¶å
            
        Returns:
            ç”Ÿæˆçš„æ–‡æ¡£è·¯å¾„
        """
        if filename is None:
            filename = f"Unit{unit}_exercises.md"
        
        md_content = []
        
        # æ ‡é¢˜
        md_content.append(f"# ğŸ“ Unit {unit} ç»ƒä¹ é¢˜")
        md_content.append("")
        md_content.append(f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        md_content.append("")
        
        # ä¸­è‹±äº’è¯‘é¢˜
        translation_exercises = exercises.get('translation', [])
        if translation_exercises:
            md_content.append("## ğŸ”„ ä¸­è‹±äº’è¯‘é¢˜")
            md_content.append("")
            
            for i, exercise in enumerate(translation_exercises, 1):
                question = exercise.get('question', '')
                answer = exercise.get('answer', '')
                exercise_type = exercise.get('type', 'zh_to_en')
                
                type_label = "ä¸­è¯‘è‹±" if exercise_type == "zh_to_en" else "è‹±è¯‘ä¸­"
                md_content.append(f"**ç¬¬{i}é¢˜** ({type_label})")
                md_content.append(f"é¢˜ç›®ï¼š{question}")
                md_content.append(f"ç­”æ¡ˆï¼š{answer}")
                md_content.append("")
        
        # å­—æ¯å¡«ç©ºé¢˜
        letter_exercises = exercises.get('letter_filling', [])
        if letter_exercises:
            md_content.append("## ğŸ”¤ å­—æ¯å¡«ç©ºé¢˜")
            md_content.append("")
            
            for i, exercise in enumerate(letter_exercises, 1):
                question = exercise.get('question', '')
                answer = exercise.get('answer', '')
                
                md_content.append(f"**ç¬¬{i}é¢˜**")
                md_content.append(f"é¢˜ç›®ï¼š{question}")
                md_content.append(f"ç­”æ¡ˆï¼š{answer}")
                md_content.append("")
        
        # çŸ­è¯­å¡«ç©ºé¢˜
        phrase_exercises = exercises.get('phrase_filling', [])
        if phrase_exercises:
            md_content.append("## ğŸ“ çŸ­è¯­å¡«ç©ºé¢˜")
            md_content.append("")
            
            for i, exercise in enumerate(phrase_exercises, 1):
                question = exercise.get('question', '')
                answer = exercise.get('answer', '')
                
                md_content.append(f"**ç¬¬{i}é¢˜**")
                md_content.append(f"é¢˜ç›®ï¼š{question}")
                md_content.append(f"ç­”æ¡ˆï¼š{answer}")
                md_content.append("")
        
        # è¯¾æ–‡é»˜å†™é¢˜
        dictation_exercises = exercises.get('dictation', [])
        if dictation_exercises:
            md_content.append("## âœï¸ è¯¾æ–‡é»˜å†™é¢˜")
            md_content.append("")
            md_content.append("æ ¹æ®ä¸­æ–‡æç¤ºï¼Œå†™å‡ºå¯¹åº”çš„è‹±æ–‡å¥å­ï¼š")
            md_content.append("")
            
            for i, exercise in enumerate(dictation_exercises, 1):
                chinese = exercise.get('chinese', '')
                english = exercise.get('english', '')
                
                md_content.append(f"**ç¬¬{i}æ®µ**")
                md_content.append(f"ä¸­æ–‡æç¤ºï¼š{chinese}")
                md_content.append(f"è‹±æ–‡ç­”æ¡ˆï¼š{english}")
                md_content.append("")
        
        # ç­”é¢˜æŒ‡å¯¼
        md_content.append("## ğŸ’¡ ç­”é¢˜æŒ‡å¯¼")
        md_content.append("")
        md_content.append("### ç­”é¢˜å»ºè®®")
        md_content.append("- ä»”ç»†é˜…è¯»é¢˜ç›®ï¼Œç†è§£è¦æ±‚")
        md_content.append("- æ³¨æ„å•è¯æ‹¼å†™å’Œè¯­æ³•æ­£ç¡®æ€§")
        md_content.append("- ç¿»è¯‘é¢˜æ³¨æ„è¯­è¨€ä¹ æƒ¯å·®å¼‚")
        md_content.append("- é»˜å†™é¢˜å¯ä»¥å…ˆåˆ—å‡ºå…³é”®è¯")
        
        # ä¿å­˜æ–‡ä»¶
        file_path = self.output_dir / "exercises" / filename
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write('\n'.join(md_content))
        
        return str(file_path)
    
    def generate_summary_index(self, processed_files: List[Dict]) -> str:
        """
        ç”Ÿæˆæ€»è§ˆç´¢å¼•æ–‡æ¡£
        
        Args:
            processed_files: å¤„ç†è¿‡çš„æ–‡ä»¶ä¿¡æ¯åˆ—è¡¨
            
        Returns:
            ç´¢å¼•æ–‡æ¡£è·¯å¾„
        """
        md_content = []
        
        # æ ‡é¢˜
        md_content.append("# ğŸ“š English Learning Assistant - æ–‡æ¡£ç´¢å¼•")
        md_content.append("")
        md_content.append(f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        md_content.append(f"**å¤„ç†æ–‡ä»¶æ•°é‡**: {len(processed_files)}")
        md_content.append("")
        
        # ç»Ÿè®¡ä¿¡æ¯
        units = set()
        total_vocabulary = 0
        
        for file_info in processed_files:
            analysis = file_info.get('analysis', {})
            if analysis.get('unit'):
                units.add(analysis['unit'])
            total_vocabulary += len(analysis.get('vocabulary', []))
        
        md_content.append("## ğŸ“Š å¤„ç†ç»Ÿè®¡")
        md_content.append("")
        md_content.append(f"- **æ¶‰åŠå•å…ƒ**: {len(units)} ä¸ª")
        md_content.append(f"- **æ€»è¯æ±‡é‡**: {total_vocabulary} ä¸ª")
        md_content.append("")
        
        # æ–‡æ¡£åˆ—è¡¨
        md_content.append("## ğŸ“‘ ç”Ÿæˆæ–‡æ¡£åˆ—è¡¨")
        md_content.append("")
        
        # æŒ‰å•å…ƒåˆ†ç»„
        lessons_by_unit = {}
        for file_info in processed_files:
            analysis = file_info.get('analysis', {})
            unit = analysis.get('unit', 'Unknown')
            if unit not in lessons_by_unit:
                lessons_by_unit[unit] = []
            lessons_by_unit[unit].append(file_info)
        
        for unit in sorted(lessons_by_unit.keys()):
            md_content.append(f"### Unit {unit}")
            md_content.append("")
            
            for file_info in lessons_by_unit[unit]:
                analysis = file_info.get('analysis', {})
                title = analysis.get('title', 'æœªå‘½å')
                lesson_path = file_info.get('lesson_path', '')
                exercise_path = file_info.get('exercise_path', '')
                
                md_content.append(f"**{title}**")
                if lesson_path:
                    md_content.append(f"- ğŸ“– [è¯¾æ–‡æ–‡æ¡£]({lesson_path})")
                if exercise_path:
                    md_content.append(f"- ğŸ“ [ç»ƒä¹ é¢˜]({exercise_path})")
                md_content.append("")
        
        # è¯æ±‡æ±‡æ€»
        md_content.append("## ğŸ“š è¯æ±‡æ–‡æ¡£")
        md_content.append("- ğŸ“‘ [è¯æ±‡æ±‡æ€»](./vocabulary/vocabulary_summary.md)")
        md_content.append("")
        
        # ä½¿ç”¨è¯´æ˜
        md_content.append("## ğŸ“– ä½¿ç”¨è¯´æ˜")
        md_content.append("")
        md_content.append("æœ¬æ–‡æ¡£ç³»ç»Ÿé€šè¿‡AIå¢å¼ºOCRæŠ€æœ¯è‡ªåŠ¨ç”Ÿæˆï¼ŒåŒ…å«ï¼š")
        md_content.append("")
        md_content.append("1. **è¯¾æ–‡æ–‡æ¡£** - åŒ…å«OCRè¯†åˆ«ã€AIæ ¡æ­£ã€å†…å®¹åˆ†æ")
        md_content.append("2. **è¯æ±‡æ±‡æ€»** - æŒ‰éš¾åº¦åˆ†çº§çš„è¯æ±‡è¡¨")
        md_content.append("3. **ç»ƒä¹ é¢˜** - å¤šç§é¢˜å‹çš„è‡ªåŠ¨ç”Ÿæˆç»ƒä¹ ")
        md_content.append("")
        md_content.append("### æŠ€æœ¯ç‰¹ç‚¹")
        md_content.append("- ğŸ¤– AIå¢å¼ºOCRè¯†åˆ«ï¼ˆå‡†ç¡®ç‡æå‡30%+ï¼‰")
        md_content.append("- ğŸ§  æ™ºèƒ½å†…å®¹åˆ†æå’Œåˆ†ç±»")
        md_content.append("- ğŸ“ è‡ªåŠ¨ä¹ é¢˜ç”Ÿæˆ")
        md_content.append("- ğŸ“š ç»“æ„åŒ–æ–‡æ¡£è¾“å‡º")
        
        # ä¿å­˜ç´¢å¼•æ–‡ä»¶
        file_path = self.output_dir / "README.md"
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write('\n'.join(md_content))
        
        return str(file_path)


def create_document_generator(output_dir: str = "output") -> MarkdownGenerator:
    """åˆ›å»ºæ–‡æ¡£ç”Ÿæˆå™¨å®ä¾‹"""
    return MarkdownGenerator(output_dir)


# æä¾›åˆ«åä»¥ä¿æŒå…¼å®¹æ€§
DocumentGenerator = MarkdownGenerator